{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import models\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from census import Census\n",
    "from us import states\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = os.path.join('data')\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_DIR, 'raw', 'tedsa_00_16_puf.csv'), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.CBSA10 != -9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.CBSA10.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_states = [2, 60, 66, 15, 72, 78]\n",
    "data = data[~data.STFIPS.isin(drop_states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25742826, 62)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "- All Data: 31,406,891 data points.\n",
    "\n",
    "## Cleaning Process\n",
    "1. Drop all where CBSA10 == -9 (no geographic information, nothing to predict). Remaining data points: 25,940,650\n",
    "\n",
    "``data = data[data.CBSA10 != -9]``\n",
    "\n",
    "2. Drop all non-contiguous USA states/territories (STFIPS: 02 Alaska, 60 Samoa, 66 Guam, 15 Hawaii, 72 Puerto Rico, 78 Virgin Islands). Remaining data points: 25,742,826\n",
    "\n",
    "``\n",
    "drop_states = [2, 60, 66, 15, 72, 78]\n",
    "data = data[~data.STFIPS.isin(drop_states)]\n",
    "``\n",
    "\n",
    "3. Drop counties that don't occur in all (17) years. Remaining data points: 22,612,190\n",
    "\n",
    "\n",
    "3. What columns to drop for the patient vector? (CASEID, YEAR)\n",
    "- Want to keep the county (CBSA10)\n",
    "\n",
    "4. What columns to drop for the county-mean vector? (CASEID, YEAR, CBSA10, STFIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_states = [2, 60, 66, 15, 72, 78]\n",
    "data = data[~data.STFIPS.isin(drop_states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Labels\n",
    "- Labels are #of admissions in a county for a given year.\n",
    "- Labels stored in dictionary, accessed with tuple (CBSA10 code, YEAR)\n",
    "- NOTE: May want to normalize based on population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#admission counts for CBSA for each year. Accessed with (CBSA, YEAR)\n",
    "\n",
    "years = sorted(data.YEAR.unique())\n",
    "cbsa_year_counts = {} \n",
    "\n",
    "for year in years:\n",
    "    counts = data[data.YEAR == year].CBSA10.value_counts()\n",
    "    for cbsa, count in counts.iteritems():\n",
    "        cbsa_year_counts[(cbsa,year)]=count\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l = []\n",
    "for k in cbsa_year_counts.keys():\n",
    "    l.append(k[0])\n",
    "    \n",
    "cbsa_counts = Counter(l)\n",
    "\n",
    "total_counts = data.CBSA10.value_counts()\n",
    "\n",
    "drop_cbsas = []\n",
    "for c in cbsa_counts:\n",
    "    if cbsa_counts[c]!=17:\n",
    "        drop_cbsas.append(c)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.CBSA10.isin(drop_cbsas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(DATA_DIR, 'cleaned', 'teds-cleaned.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxillary Mean Features\n",
    "mean_drop_cols = ['CASEID', 'YEAR', 'CBSA10']\n",
    "cbsa_year_means = {}\n",
    "for year in years:\n",
    "    year_df = data[data.YEAR == year]\n",
    "    cbsas = year_df.CBSA10.unique()\n",
    "    for cbsa in cbsas:\n",
    "        means = year_df[year_df.CBSA10==cbsa].mean().drop(mean_drop_cols)\n",
    "        cbsa_year_means[(cbsa,year)]=np.array(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute and Save Admission Counts and Feature Means to Pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned/cbsa-means.pkl', 'wb') as output:\n",
    "    pickle.dump(cbsa_year_means, output)\n",
    "    \n",
    "with open('data/cleaned/admission-counts.pkl', 'wb') as output:\n",
    "    pickle.dump(cbsa_year_counts, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsa_year_means = {}\n",
    "with open('test.pkl', 'rb') as inp:\n",
    "    cbsa_year_means = pickle.load(inp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sequence Vectors\n",
    "- Sequences are constructed as follows: For each patient admission entry, we construct N vectors that are the admission entry concatenated with the county mean features for 1 of N years. Sequences are these N vectors ordered by year. \n",
    "- Labels are admission counts for the corresponding county in the patient entry.\n",
    "\n",
    "- Baseline model could just use sequences of county features of each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient_drop_cols = ['CASEID', 'YEAR']\n",
    "def feature_vector(pandas_row, year):\n",
    "    cb_year = (row.CBSA10, year)\n",
    "    patient_vec = np.array(row.drop(patient_drop_cols))\n",
    "    means_vec = cbsa_year_means[cb_year]\n",
    "    \n",
    "#     feat_vec = np.concatenate((patient_vec, means_vec))\n",
    "    feat_vec = means_vec\n",
    "#     feat_vec = np.concatenate(([cb_year[0], cb_year[1]], means_vec))\n",
    "    \n",
    "    \n",
    "    label = cbsa_year_counts[cb_year]\n",
    "    return feat_vec, label\n",
    "\n",
    "\n",
    "test_df = data.sample(n=5000)\n",
    "X,Y=[],[]\n",
    "for idx, row in test_df.iterrows():\n",
    "    seq = []\n",
    "    for year in years[:-1]:\n",
    "        x,y = feature_vector(row, year)\n",
    "        seq.append(x)\n",
    "    \n",
    "    X.append(np.array(seq))\n",
    "    x,y = feature_vector(row, years[-1])\n",
    "    l = 0\n",
    "    if y > 10:\n",
    "        l = 1\n",
    "    Y.append(l)\n",
    "    \n",
    "    \n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# Y = Y/np.max(Y)\n",
    "Y = keras.utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Population data and create dict: CBSA -> Population (Year: 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_fips(state, county):\n",
    "    state = str(state)\n",
    "    county = str(county)\n",
    "    if len(state)<2:\n",
    "        state = \"0\"*(2-len(state))+state\n",
    "    if len(county)<3:\n",
    "        county = \"0\"*(3-len(county))+county\n",
    "    \n",
    "    return state + county\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in CBSA > FIPS Translation values for plotting purposes (and geolocation)\n",
    "cbsa_fips = pd.read_excel(os.path.join(DATA_DIR, 'raw', \"CBSA-FIPS.xls\"), header=2,nrows=1882,convert_float=False, dtype='object')\n",
    "\n",
    "\n",
    "#CBSA -> [FIPS]\n",
    "cbsa2fips = {}\n",
    "all_cbsas = data.CBSA10.unique()\n",
    "for idx,row in cbsa_fips.iterrows():\n",
    "    cb = int(row['CBSA Code'])\n",
    "    if cb in all_cbsas:\n",
    "        fips = fix_fips(row['FIPS State Code'], row['FIPS County Code'])\n",
    "        if not (cb in cbsa2fips):\n",
    "            cbsa2fips[cb] = [fips]\n",
    "        else:\n",
    "            cbsa2fips[cb].append(fips)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsa2fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = Census(\"846367a9aca0d09de354714a8eb90669e7ec5837\",year=2010)\n",
    "pop_results = c.sf1.state_county('P001001', Census.ALL,Census.ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_pops = {}\n",
    "for p in pop_results:\n",
    "    fips = p['state']+p['county']\n",
    "    pop = int(p['P001001'])\n",
    "    fips_pops[fips]=pop\n",
    "    \n",
    "cbsa_pops = {}\n",
    "for cb in cbsa2fips:\n",
    "    fips = cbsa2fips[cb]\n",
    "    s = 0\n",
    "    for f in fips:\n",
    "        s += fips_pops[f]\n",
    "    cbsa_pops[cb]=s  \n",
    "    \n",
    "pops = pd.DataFrame({'CBSA': list(cbsa_pops.keys()), 'Pop': list(cbsa_pops.values())})\n",
    "pops.to_csv('data/cleaned/cbsa-populations-2010.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/county_adjacency.txt', 'rb') as f:\n",
    "    lines = [line.decode('utf-8', errors='replace').strip().split('\\t') for line in f.readlines()]\n",
    "\n",
    "neighbors = []\n",
    "current = None\n",
    "for line in lines:\n",
    "    if len(line) == 4:\n",
    "        current = line[1]\n",
    "        try:\n",
    "            pair = (current, line[3])\n",
    "        except IndexError:\n",
    "            print(line)\n",
    "    elif len(line) == 2:\n",
    "        pair = (current, line[1])\n",
    "    elif len(line) == 3:\n",
    "        current = line[0]\n",
    "        pair = (current, line[2])\n",
    "    \n",
    "    alt_pair = (pair[1], pair[0])\n",
    "    \n",
    "    if (pair not in neighbors) and (alt_pair not in neighbors) and (pair[0] != pair[1]): \n",
    "        neighbors.append(pair)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
